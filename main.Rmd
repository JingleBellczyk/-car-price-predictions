---
---
---

### Title: Car price prediction.

### authors: Agata Belczyk, Jakub Ner

```{r warning=FALSE}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}
if (!requireNamespace("car", quietly = TRUE)) {
  install.packages("car")
}
if (!require(psych)) {
  install.packages("psych")
}
# Load necessary libraries
library(tidyverse)  # Includes ggplot2, dplyr, and readr
library(caret)      # For modeling and preprocessing data
library(car)        # Contains functions for VIF (Variance Inflation Factor)
library(psych) 

# Plot settings
theme_set(theme_minimal())

# Load data from CSV file
data <- read_csv("car_dataset.csv", show_col_types = FALSE)
beggining_data_length <- nrow(data)
data

numeric_data <- data[, sapply(data, is.numeric)]

# statistics about numeric data
describe(numeric_data)

```

### Clean up data, by dropping rows with null values or duplicated rows.

```{r}
# Remove rows with missing values
data <- na.omit(data) # 320 rows dropped 

# Remove duplicate rows and reset index
data <- data[!duplicated(data), ] # 50 rows dropped
rownames(data) <- NULL

# Display summary statistics for all variables
data
```

### Histograms for each feature

```{r}
# Load necessary libraries
library(tidyverse)  # Includes ggplot2, dplyr, and readr

# Numeric columns (assuming they contain only numeric data)
numeric_columns <- c("Price", "Mileage", "Year")  # Replace ... with actual column names

# Non-numeric columns (assuming they contain at least one non-numeric value)
non_numeric_columns <- c("Brand", "Body", "EngineV", "Engine Type", "Registration","Model")  # Replace ... with actual column names

# Define the number of rows of plots for each type
n_numeric <- length(numeric_columns)
n_non_numeric <- length(non_numeric_columns)

# Get the column names of the data
all_columns <- colnames(data)

# Function to get ylim for histograms
get_hist_ylim <- function(column_data) {
  hist_info <- hist(column_data, plot = FALSE)
  return(c(3, max(hist_info$counts) * 1.2))  # Adding some space above the highest bar
}

# Function to get ylim for bar plots
get_barplot_ylim <- function(column_data) {
  bar_info <- table(column_data)
  return(c(0, max(bar_info) * 1.2))  # Adding some space above the highest bar
}

# Plot histograms for numeric columns
for (i in 1:n_numeric) {
  column_data <- data[[numeric_columns[i]]]
  hist(column_data, main = numeric_columns[i], xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")
}

# Plot bar plots for non-numeric columns
for (i in 1:n_non_numeric) {
  column_data <- data[[non_numeric_columns[i]]]
  barplot(table(column_data), main = non_numeric_columns[i], xlab = "", col = "lightblue", border = "black", ylim = get_barplot_ylim(column_data), las=2,ylab="Count")
}

data
```

At first glance we can see that Mlieage, EngineV, Year and Models have outliers, so they don''t have predictice power. Therefore we remove rows where values are greater than 99th percentile.

```{r}

column_data <- data[["Mileage"]]
hist(column_data, main = "Mileage", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")
#------------------------------------------------
q <- quantile(data$Mileage, 0.99)
data <- data[-which(data$Mileage > q), ]  # Remove rows where Mileage is greater than 99th percentile
data <- data[order(data$Mileage), ]  # Reset index
rownames(data) <- NULL  # Reset row names

#------------------------------------------------
column_data <- data[["Mileage"]]
hist(column_data, main = "Mileage", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Display detailed descriptive statistics for numeric columns only
describe(numeric_data)

```

### We do the same for Year

```{r}

column_data <- data[["Year"]]
hist(column_data, main = "Year", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")
#----------------------------
# Calculate the 1st percentile for the 'Year' column
q <- quantile(data$Year, 0.01)

# Remove rows where 'Year' is less than the 1st percentile
data <- data[data$Year >= q, ]
data <- data[order(data$Year), ]  # Reset index
rownames(data) <- NULL  # Reset row names

#----------------------------
# Display summary statistics
column_data <- data[["Year"]]
hist(column_data, main = "Year", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Display detailed descriptive statistics for numeric columns only
describe(numeric_data)
```

### We check how many models occurs less than 50 times

```{r}
# Create a histogram for the 'Model' column
histogram <- data.frame(Model = data$Model)
histogram$Count <- rep(1, nrow(data))

# Group by 'Model' and count the occurrences
histogram <- aggregate(Count ~ Model, data = histogram, FUN = length)
q <- 50 #times of occuring

# Find models with a count less than the threshold 'q'
models_to_drop <- histogram$Model[histogram$Count < q]

# Get the number of models to drop
length(models_to_drop)


```

### 272 models appear less than 50 times, that's why I we dropped Model's column

```{r}
# Remove the 'Model' column from the data frame
data <- data[, !names(data) %in% "Model"]
```

### According to Google, the biggest EngineV which exist is equal to 8,3l, so we drop all values bigger than that value

```{r warning=FALSE}
# Install and load the 'psych' package for more detailed summary statistics
install.packages("psych")
library(psych)

# Assuming you have a dataframe named 'data'

# Remove rows where 'EngineV' is greater than 8.2
data <- data[data$EngineV <= 8.2, ]

# Reset the row indices
rownames(data) <- NULL

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Display detailed descriptive statistics for numeric columns only
describe(numeric_data)
```

```{r}
# Calculate the difference
print(beggining_data_length-nrow(data))

# Division and expressing the result as a percentage
percentage <- ((beggining_data_length-nrow(data)) / nrow(data)) * 100
print(percentage) 

```

### *In total we have dropped 12% of rows - 467*

# Now when data is cleaned We can looks for in- and dependent variables

```{r}
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
x <- select(data, -Price)
y <- log(data$Price)

```

```{r warning=FALSE}
# Load necessary libraries
library(ggplot2)

# Number of rows of plots
n <- 4

# Extract the columns from 'x'
columns <- colnames(x)

# Create a list to store the plots
plot_list <- list()

# Loop through the columns and create scatter plots
for (i in 1:length(columns)) {
  if (i > length(columns)) break # Break if there are no more columns
  column <- columns[i]
  p <- ggplot(data, aes_string(x = paste0('`', column, '`'), y = 'log(Price)')) +
    geom_point() + # Create scatter plot
    xlab(NULL) + # Remove x-axis label
    ggtitle(paste(column, "and Price")) # Set plot title
  plot_list[[i]] <- p # Store the plot in the list
}

# Print each plot separately
for (i in 1:length(plot_list)) {
  print(plot_list[[i]])
}



```

### Firstly we made plots of each variable and Price, but plots show much more when we log(Price).

### Plot of Model and log(Price) doesn't show any pattern, but other plots with continues variables looks promising.

```{r}
# Selecting independent continuous variables
continues_independent <- data[c("Mileage", "EngineV", "Year")]

# Performing F-test to obtain p-values
p_values <- round(summary(lm(log(Price) ~ Mileage + EngineV + Year, data))$coefficients[, 4], 3)


# Outputting the selected independent continuous variables
print(continues_independent)

```

### Changing categorical features to numerical.

```{r warning=FALSE}
# Install and load the 'recipes' package if not already installed
if (!require(recipes)) {
  install.packages("recipes")
}
library(recipes)

# Create a recipe
rec <- recipe(~., data = x) %>%
  step_dummy(all_nominal(), one_hot = TRUE)

# Prepare the recipe
rec <- prep(rec)

# Apply the recipe to create dummy variables
x_with_dummies <- bake(rec, new_data = x)

# Display the first 3 rows of the resulting data frame
head(x_with_dummies, 3)

```

### Standarization

```{r}
# Install and load the 'recipes' package if not already installed
if (!require(recipes)) {
  install.packages("recipes")
}
library(recipes)

# Create a recipe
rec <- recipe(~., data = x_with_dummies) %>%
  step_scale(all_predictors())

# Prepare the recipe
rec <- prep(rec)

# Apply the recipe to scale the data
scaled_inputs <- bake(rec, new_data = x_with_dummies)

# Display the first 3 rows of the resulting scaled data frame
head(scaled_inputs, 3)

```

### Setting seed

```{r}
# Install and load the 'caret' package if not already installed
if (!require(caret)) {
  install.packages("caret")
}
library(caret)

# Set the seed for reproducibility
set.seed(42)

# Split the data into training and testing sets
split <- createDataPartition(y, p = 0.8, list = FALSE)
x_train <- scaled_inputs[split, ]
x_test <- scaled_inputs[-split, ]
y_train <- y[split]
y_test <- y[-split]

```

```{r warning=FALSE}

df_pf <- data.frame(Predicted = numeric(774), Target = numeric(774), Difference = numeric(774))

reg <- lm(y_train ~ ., data = x_train)
df_pf$Predicted <- exp(predict(reg, newdata = x_test))

# Convert target values back from log scale
df_pf$Target <- exp(y_test)

# Calculate absolute percentage difference
df_pf$Difference <- abs(df_pf$Predicted - df_pf$Target) / df_pf$Target * 100

# Sort dataframe by difference
df_pf <- df_pf[order(df_pf$Difference),]

# Reset row names
rownames(df_pf) <- NULL

# Display first few rows
df_pf
```

```{r warning=FALSE}
# Install and load ggplot2 if not already installed
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}

# Create predictions
predicted_values <- predict(reg, newdata = x_test)

# Create a data frame for plotting
plot_data <- data.frame(Predicted = predicted_values, Actual = y_test)

# Create the scatter plot and line plot
ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(x = "Predicted values", y = "Test values") +
  theme_minimal()

```

### This is a regression model. We can see that predicted values overlaps with real values. In the next steps we will see how well model works.

```{r warning=FALSE}
# Install and load necessary packages if not already installed
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require(grid)) {
  install.packages("grid")
  library(grid)
}

# Assuming you have already fitted a model 'reg' and have test data 'x_test' and 'y_test'

# Create predictions
predicted_values <- predict(reg, newdata = x_test)

# Calculate residuals
residuals <- y_test - predicted_values

# Create a data frame for plotting
plot_data <- data.frame(Residuals = residuals)

# Create the histogram with density plot and a vertical line at zero
ggplot(plot_data, aes(x = Residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "white") +
  geom_density(color = "blue") +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  ggtitle("Residuals PDF") +
  theme_minimal()

```

### A p-value close to 0 (typically less than 0.05) indicates that the null hypothesis, which posits that there is no effect or relationship, can be rejected. This suggests a significant relationship between the predictor and the response variable. Essentially, low p-values imply that the predictor variables are making a meaningful contribution to the model by explaining the variance in the response variable:

```{r warning=FALSE}
# Assuming you have already fitted a model 'reg' and have test data 'x_test' and 'y_test'

# Create predictions
predicted_values <- predict(reg, newdata = x_test)

# Calculate residuals
residuals <- y_test - predicted_values

# Fit a linear model with residuals as the predictor and y_test as the response
model <- lm(y_test ~ residuals)

# Extract p-values
p_values <- summary(model)$coefficients[, "Pr(>|t|)"]

# Round the p-values to 3 decimal places
#p_values <- round(p_values, 3)

# Print p-values
p_values

```

### We load libraries

```{r warning=FALSE}
# Install and load necessary packages
if (!require(e1071)) {
  install.packages("e1071")
  library(e1071)
}
if (!require(randomForest)) {
  install.packages("randomForest")
  library(randomForest)
}
if (!require(xgboost)) {
  install.packages("xgboost")
  library(xgboost)
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}
```

This code aims to evaluate the performance of various regression models on a given dataset. The models included in this evaluation are Linear Regression, Generalized Linear Model (Polynomial Regression), Support Vector Regression (SVR), Random Forest, and XGBoost.

### **Generalized Linear Model (Polynomial Regression):**

Polynomial features (squared terms) are generated for the training and test datasets. A linear model is trained using these features, and its performance is evaluated using MSE on the test data.

### **Support Vector Regression (SVR):**

An SVR model with a radial basis function (RBF) kernel is trained on the training data. The model's accuracy is assessed by computing the MSE on the test data.

### **Random Forest:**

A Random Forest model with 100 decision trees is trained on the training data. Its performance is evaluated by calculating the MSE on the test data.

### **XGBoost:**

The datasets are converted to DMatrix format for XGBoost. An XGBoost model is trained with specific parameters, and its performance is assessed using MSE on the test data.

```{r warning=FALSE}

# Set the seed for reproducibility
set.seed(42)

# Assuming x_train, y_train, x_test, y_test are already defined as data.frames

# Linear Regression
reg <- lm(y_train ~ ., data = data.frame(x_train, y_train))
MSE_reg <- mean((predict(reg, data.frame(x_test)) - y_test)^2)
cat(sprintf('Błąd średniokwadratowy modelu liniowego: %.3f\n', MSE_reg))

# Generalized Linear Model (Polynomial Regression)
# Create polynomial features manually
poly_features <- function(data, degree) {
  poly_data <- data.frame(intercept = rep(1, nrow(data)))
  for (col in names(data)) {
    for (d in 1:degree) {
      poly_data[[paste(col, "poly", d, sep = "_")]] <- data[[col]]^d
    }
  }
  return(poly_data)
}

gen_features_train <- poly_features(data.frame(x_train), degree = 2)
model_GLM <- lm(y_train ~ ., data = data.frame(gen_features_train, y_train))

gen_features_test <- poly_features(data.frame(x_test), degree = 2)
MSE_GLM <- mean((predict(model_GLM, data.frame(gen_features_test)) - y_test)^2)
cat(sprintf('Mean squared error for model GLM: %.3f\n', MSE_GLM))

# Support Vector Regression
model_svr <- svm(y_train ~ ., data = data.frame(x_train, y_train), kernel = 'radial', cost = 3)
MSE_SVR <- mean((predict(model_svr, data.frame(x_test)) - y_test)^2)
cat(sprintf('Mean squared error for model  SVR: %.3f\n', MSE_SVR))

# Random Forest
model_rf <- randomForest(y_train ~ ., data = data.frame(x_train, y_train), ntree = 100, random_state = 42)
MSE_RF <- mean((predict(model_rf, data.frame(x_test)) - y_test)^2)
cat(sprintf('Mean squared error for model RF: %.3f\n', MSE_RF))

# XGBoost
D_train <- xgb.DMatrix(data = as.matrix(x_train), label = y_train)
D_test <- xgb.DMatrix(data = as.matrix(x_test), label = y_test)
param <- list(max_depth = 30, eta = 1, objective = "reg:squarederror")
model_xgb <- xgb.train(param, D_train, nrounds = 2)
MSE_XGB <- mean((predict(model_xgb, D_test) - y_test)^2)
cat(sprintf('Mean squared error for model XGB: %.3f\n', MSE_XGB))

# Visualization for each model
# Function to create scatter plot
create_plot <- function(predicted, actual, title) {
  plot_data <- data.frame(Predicted = predicted, Actual = actual)
  ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    ggtitle(title) +
    xlab('Predicted values') +
    ylab('Test values') +
    theme_minimal()
}

# Linear Regression plot
create_plot(predict(reg, data.frame(x_test)), y_test, 'Linear Regression')

# Generalized Linear Model plot
create_plot(predict(model_GLM, data.frame(gen_features_test)), y_test, 'Generalized Linear Model')

# Support Vector Regression plot
create_plot(predict(model_svr, data.frame(x_test)), y_test, 'Support Vector Machine')

# Random Forest plot
create_plot(predict(model_rf, data.frame(x_test)), y_test, 'Random Forest')

# XGBoost plot
create_plot(predict(model_xgb, D_test), y_test, 'XGBoost')
```

```{r}
# Calculate R-squared and adjusted R-squared for Linear Regression
summary_reg <- summary(reg)
r2 <- summary_reg$r.squared
adjusted_r2 <- summary_reg$adj.r.squared

cat(sprintf("R^2 and Adjusted R^2 Linear Regression: %.3f, %.3f\n", r2, adjusted_r2))

```

-   The Linear Regression model shows an R-squared value of 0.883 and an adjusted R-squared value of 0.882. These values indicate that the model explains approximately 88.3% of the variance in the dependent variable. The small difference between the R-squared and adjusted R-squared values suggests that the model does not overfit significantly, maintaining a balance between complexity and predictive power. This result demonstrates that even a simple linear model can capture a substantial portion of the data's variability. However, it may not capture non-linear relationships as effectively as the polynomial regression model.

    ```{r}
    # Fit the Generalized Linear Model (Polynomial Regression)
    model_GLM <- lm(y_train ~ ., data = gen_features_train)

    # Get summary of the model
    summary_GLM <- summary(model_GLM)

    # Extract R-squared and adjusted R-squared
    r2_GLM <- summary_GLM$r.squared
    adjusted_r2_GLM <- summary_GLM$adj.r.squared

    cat(sprintf("R^2 and Adjusted R^2 Generalized Linear Model: %.3f, %.3f\n", r2_GLM, adjusted_r2_GLM))

    ```

    The polynomial regression model with manually created squared features shows high R-squared (0.894) and adjusted R-squared (0.893) values. This indicates that the model effectively captures the variance in the data, suggesting a **good fit and strong predictive power**. The small difference between R-squared and adjusted R-squared implies minimal overfitting. The model's performance highlights the benefit of adding polynomial terms to capture non-linear relationships in the data.
