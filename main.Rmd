```{r warning=FALSE}
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}
if (!requireNamespace("caret", quietly = TRUE)) {
  install.packages("caret")
}
if (!requireNamespace("car", quietly = TRUE)) {
  install.packages("car")
}
if (!require(psych)) {
  install.packages("psych")
}
# Load necessary libraries
library(tidyverse)  # Includes ggplot2, dplyr, and readr
library(caret)      # For modeling and preprocessing data
library(car)        # Contains functions for VIF (Variance Inflation Factor)
library(psych) 

# Plot settings
theme_set(theme_minimal())

# Load data from CSV file
data <- read_csv("car_dataset.csv", show_col_types = FALSE)
beggining_data_length <- nrow(data)
data

numeric_data <- data[, sapply(data, is.numeric)]

# statistics about numeric data
describe(numeric_data)

```

### Clean up data, by dropping rows with null values or duplicated rows.

```{r}
# Remove rows with missing values
data <- na.omit(data) # 320 rows dropped 

# Remove duplicate rows and reset index
data <- data[!duplicated(data), ] # 50 rows dropped
rownames(data) <- NULL

# Display summary statistics for all variables
data
```

### Histrograms for each feature

```{r}
# Load necessary libraries
library(tidyverse)  # Includes ggplot2, dplyr, and readr

# Numeric columns (assuming they contain only numeric data)
numeric_columns <- c("Price", "Mileage", "Year")  # Replace ... with actual column names

# Non-numeric columns (assuming they contain at least one non-numeric value)
non_numeric_columns <- c("Brand", "Body", "EngineV", "Engine Type", "Registration","Model")  # Replace ... with actual column names

# Define the number of rows of plots for each type
n_numeric <- length(numeric_columns)
n_non_numeric <- length(non_numeric_columns)

# Get the column names of the data
all_columns <- colnames(data)

# Function to get ylim for histograms
get_hist_ylim <- function(column_data) {
  hist_info <- hist(column_data, plot = FALSE)
  return(c(3, max(hist_info$counts) * 1.2))  # Adding some space above the highest bar
}

# Function to get ylim for bar plots
get_barplot_ylim <- function(column_data) {
  bar_info <- table(column_data)
  return(c(0, max(bar_info) * 1.2))  # Adding some space above the highest bar
}

# Plot histograms for numeric columns
for (i in 1:n_numeric) {
  column_data <- data[[numeric_columns[i]]]
  hist(column_data, main = numeric_columns[i], xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")
}

# Plot bar plots for non-numeric columns
for (i in 1:n_non_numeric) {
  column_data <- data[[non_numeric_columns[i]]]
  barplot(table(column_data), main = non_numeric_columns[i], xlab = "", col = "lightblue", border = "black", ylim = get_barplot_ylim(column_data), las=2,ylab="Count")
}

data
```

### At first glance we can see that Mlieage, EngineV, Year and Models have outliers, so they don''t have predictice power. Therefore we remove rows where values are greater than 99th percentile.

```{r}

column_data <- data[["Mileage"]]
hist(column_data, main = "Mileage", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")
#------------------------------------------------
q <- quantile(data$Mileage, 0.99)
data <- data[-which(data$Mileage > q), ]  # Remove rows where Mileage is greater than 99th percentile
data <- data[order(data$Mileage), ]  # Reset index
rownames(data) <- NULL  # Reset row names

#------------------------------------------------
column_data <- data[["Mileage"]]
hist(column_data, main = "Mileage", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Display detailed descriptive statistics for numeric columns only
describe(numeric_data)

```

### We do the same for Year

```{r}

column_data <- data[["Year"]]
hist(column_data, main = "Year", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")
#----------------------------
# Calculate the 1st percentile for the 'Year' column
q <- quantile(data$Year, 0.01)

# Remove rows where 'Year' is less than the 1st percentile
data <- data[data$Year >= q, ]
data <- data[order(data$Year), ]  # Reset index
rownames(data) <- NULL  # Reset row names

#----------------------------
# Display summary statistics
column_data <- data[["Year"]]
hist(column_data, main = "Year", xlab = "", col = "lightblue", border = "black", ylim = get_hist_ylim(column_data),ylab="Count")

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Display detailed descriptive statistics for numeric columns only
describe(numeric_data)
```

### We check how many models occurs less than 50 times

```{r}
# Create a histogram for the 'Model' column
histogram <- data.frame(Model = data$Model)
histogram$Count <- rep(1, nrow(data))

# Group by 'Model' and count the occurrences
histogram <- aggregate(Count ~ Model, data = histogram, FUN = length)
q <- 50 #times of occuring

# Find models with a count less than the threshold 'q'
models_to_drop <- histogram$Model[histogram$Count < q]

# Get the number of models to drop
length(models_to_drop)


```

```{r}

```

### 273 models appear less than 50 times, that's why I we dropped Model's column

```{r}
# Remove the 'Model' column from the data frame
data <- data[, !names(data) %in% "Model"]
```

### According to Google, the biggest EngineV which exist is equal to 8,3l, so we drop all values bigger than that value

```{r warning=FALSE}
# Install and load the 'psych' package for more detailed summary statistics
install.packages("psych")
library(psych)

# Assuming you have a dataframe named 'data'

# Remove rows where 'EngineV' is greater than 8.2
data <- data[data$EngineV <= 8.2, ]

# Reset the row indices
rownames(data) <- NULL

# Select only numeric columns
numeric_data <- data[sapply(data, is.numeric)]

# Display detailed descriptive statistics for numeric columns only
describe(numeric_data)
data
```

```{r}
print(beggining_data_length-nrow(data))
# Calculate the difference
difference <- 4345 - 3798
print(difference) # 547

# Division and expressing the result as a percentage
percentage <- (difference / 4345) * 100
print(percentage) # 12.6

```

### *In total we have dropped 13% of rows - 547*

# Now when data is cleaned I can looks for in- and dependent variables

```{r}
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
x <- select(data, -Price)
y <- log(data$Price)

```

```{r warning=FALSE}
# Load necessary libraries
library(ggplot2)

# Number of rows of plots
n <- 4

# Extract the columns from 'x'
columns <- colnames(x)

# Create a list to store the plots
plot_list <- list()

# Loop through the columns and create scatter plots
for (i in 1:length(columns)) {
  if (i > length(columns)) break # Break if there are no more columns
  column <- columns[i]
  p <- ggplot(data, aes_string(x = paste0('`', column, '`'), y = 'log(Price)')) +
    geom_point() + # Create scatter plot
    xlab(NULL) + # Remove x-axis label
    ggtitle(paste(column, "and Price")) # Set plot title
  plot_list[[i]] <- p # Store the plot in the list
}

# Print each plot separately
for (i in 1:length(plot_list)) {
  print(plot_list[[i]])
}



```

Firstly I made plots of each variable and Price but plots show much more when we log(Price).

Plot of Model and log(Price) doesn't show any pattern, but other plots with continues variables looks promising.

```{r}
# Selecting independent continuous variables
continues_independent <- data[c("Mileage", "EngineV", "Year")]

# Performing F-test to obtain p-values
p_values <- round(summary(lm(log(Price) ~ Mileage + EngineV + Year, data))$coefficients[, 4], 3)


# Outputting the selected independent continuous variables
print(continues_independent)

```

Changing categorical features to numerical.

```{r warning=FALSE}
# Install and load the 'recipes' package if not already installed
if (!require(recipes)) {
  install.packages("recipes")
}
library(recipes)

# Create a recipe
rec <- recipe(~., data = x) %>%
  step_dummy(all_nominal(), one_hot = TRUE)

# Prepare the recipe
rec <- prep(rec)

# Apply the recipe to create dummy variables
x_with_dummies <- bake(rec, new_data = x)

# Display the first 3 rows of the resulting data frame
head(x_with_dummies, 3)

```

## time for standarization

```{r}
# Install and load the 'recipes' package if not already installed
if (!require(recipes)) {
  install.packages("recipes")
}
library(recipes)

# Create a recipe
rec <- recipe(~., data = x_with_dummies) %>%
  step_scale(all_predictors())

# Prepare the recipe
rec <- prep(rec)

# Apply the recipe to scale the data
scaled_inputs <- bake(rec, new_data = x_with_dummies)

# Display the first 3 rows of the resulting scaled data frame
head(scaled_inputs, 3)

```

setting seed

```{r}
# Install and load the 'caret' package if not already installed
if (!require(caret)) {
  install.packages("caret")
}
library(caret)

# Set the seed for reproducibility
set.seed(42)

# Split the data into training and testing sets
split <- createDataPartition(y, p = 0.8, list = FALSE)
x_train <- scaled_inputs[split, ]
x_test <- scaled_inputs[-split, ]
y_train <- y[split]
y_test <- y[-split]

```

```{r}
data
```

```{r warning=FALSE}

df_pf <- data.frame(Predicted = numeric(774), Target = numeric(774), Difference = numeric(774))

reg <- lm(y_train ~ ., data = x_train)
df_pf$Predicted <- exp(predict(reg, newdata = x_test))

# Convert target values back from log scale
df_pf$Target <- exp(y_test)

# Calculate absolute percentage difference
df_pf$Difference <- abs(df_pf$Predicted - df_pf$Target) / df_pf$Target * 100

# Sort dataframe by difference
df_pf <- df_pf[order(df_pf$Difference),]

# Reset row names
rownames(df_pf) <- NULL

# Display first few rows
df_pf

```

```{r}
# Fit the linear regression model
reg <- lm(y_train ~ ., data = x_train)

# Calculate R^2
r2 <- summary(reg)$r.squared

# Number of observations
n <- nrow(x_train)

# Number of predictors
p <- ncol(x_train)

# Calculate adjusted R^2
adjusted_r2 <- 1 - (1 - r2) * (n - 1) / (n - p - 1)

# Print R^2 and adjusted R^2
cat("R^2 and Adjusted R^2: ", round(r2, 3), round(adjusted_r2, 3), "\n")

```

```{r warning=FALSE}
# Install and load ggplot2 if not already installed
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}

# Assuming you have already fitted a model 'reg' and have test data 'x_test' and 'y_test'

# Create predictions
predicted_values <- predict(reg, newdata = x_test)

# Create a data frame for plotting
plot_data <- data.frame(Predicted = predicted_values, Actual = y_test)

# Create the scatter plot and line plot
ggplot(plot_data, aes(x = Predicted, y = Actual)) +
  geom_point(alpha = 0.3) +
  geom_abline(intercept = 0, slope = 1, color = "red") +
  labs(x = "Predicted values", y = "Test values") +
  theme_minimal()

```

```{r warning=FALSE}
# Install and load necessary packages if not already installed
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require(grid)) {
  install.packages("grid")
  library(grid)
}

# Assuming you have already fitted a model 'reg' and have test data 'x_test' and 'y_test'

# Create predictions
predicted_values <- predict(reg, newdata = x_test)

# Calculate residuals
residuals <- y_test - predicted_values

# Create a data frame for plotting
plot_data <- data.frame(Residuals = residuals)

# Create the histogram with density plot and a vertical line at zero
ggplot(plot_data, aes(x = Residuals)) +
  geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "white") +
  geom_density(color = "blue") +
  geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
  ggtitle("Residuals PDF") +
  theme_minimal()

```

```{r}
# Assuming you have already fitted a model 'reg' and have test data 'x_test' and 'y_test'

# Create predictions
predicted_values <- predict(reg, newdata = x_test)

# Calculate residuals
residuals <- y_test - predicted_values

# Fit a linear model with residuals as the predictor and y_test as the response
model <- lm(y_test ~ residuals)

# Extract p-values
p_values <- summary(model)$coefficients[, "Pr(>|t|)"]

# Round the p-values to 3 decimal places
#p_values <- round(p_values, 3)

# Print p-values
p_values

```

A p-value close to 0 (typically less than 0.05) indicates that the null hypothesis, which posits that there is no effect or relationship, can be rejected. This suggests a significant relationship between the predictor and the response variable. Essentially, low p-values imply that the predictor variables are making a meaningful contribution to the model by explaining the variance in the response variable.

```{r}
# Install and load necessary packages
if (!require(caret)) {
  install.packages("caret")
  library(caret)
}
if (!require(e1071)) {
  install.packages("e1071")
  library(e1071)
}
if (!require(randomForest)) {
  install.packages("randomForest")
  library(randomForest)
}
if (!require(xgboost)) {
  install.packages("xgboost")
  library(xgboost)
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}
```

```{r warning=FALSE}

# Set the seed for reproducibility
set.seed(42)

# Assuming x_train, y_train, x_test, y_test are already defined as data.frames

# Linear Regression
reg <- lm(y_train ~ ., data = data.frame(x_train, y_train))
MSE_reg <- mean((predict(reg, data.frame(x_test)) - y_test)^2)
cat(sprintf('Błąd średniokwadratowy modelu liniowego: %.3f\n', MSE_reg))

# Generalized Linear Model (Polynomial Regression)
# Create polynomial features manually
poly_features <- function(data, degree) {
  poly_data <- data.frame(intercept = rep(1, nrow(data)))
  for (col in names(data)) {
    for (d in 1:degree) {
      poly_data[[paste(col, "poly", d, sep = "_")]] <- data[[col]]^d
    }
  }
  return(poly_data)
}

gen_features_train <- poly_features(data.frame(x_train), degree = 2)
model_GLM <- lm(y_train ~ ., data = data.frame(gen_features_train, y_train))

gen_features_test <- poly_features(data.frame(x_test), degree = 2)
MSE_GLM <- mean((predict(model_GLM, data.frame(gen_features_test)) - y_test)^2)
cat(sprintf('Błąd średniokwadratowy modelu GLM: %.3f\n', MSE_GLM))

# Support Vector Regression
model_svr <- svm(y_train ~ ., data = data.frame(x_train, y_train), kernel = 'radial', cost = 3)
MSE_SVR <- mean((predict(model_svr, data.frame(x_test)) - y_test)^2)
cat(sprintf('Błąd średniokwadratowy modelu SVR: %.3f\n', MSE_SVR))

# Random Forest
model_rf <- randomForest(y_train ~ ., data = data.frame(x_train, y_train), ntree = 100, random_state = 42)
MSE_RF <- mean((predict(model_rf, data.frame(x_test)) - y_test)^2)
cat(sprintf('Błąd średniokwadratowy modelu RF: %.3f\n', MSE_RF))

# XGBoost
D_train <- xgb.DMatrix(data = as.matrix(x_train), label = y_train)
D_test <- xgb.DMatrix(data = as.matrix(x_test), label = y_test)
param <- list(max_depth = 30, eta = 1, objective = "reg:squarederror")
model_xgb <- xgb.train(param, D_train, nrounds = 2)
MSE_XGB <- mean((predict(model_xgb, D_test) - y_test)^2)
cat(sprintf('Błąd średniokwadratowy modelu XGB: %.3f\n', MSE_XGB))

# Visualization for each model
# Function to create scatter plot
create_plot <- function(predicted, actual, title) {
  plot_data <- data.frame(Predicted = predicted, Actual = actual)
  ggplot(plot_data, aes(x = Predicted, y = Actual)) +
    geom_point(alpha = 0.3) +
    geom_abline(intercept = 0, slope = 1, color = "red") +
    ggtitle(title) +
    xlab('Predicted values') +
    ylab('Test values') +
    theme_minimal()
}

# Linear Regression plot
create_plot(predict(reg, data.frame(x_test)), y_test, 'Linear Regression')

# Generalized Linear Model plot
create_plot(predict(model_GLM, data.frame(gen_features_test)), y_test, 'Generalized Linear Model')

# Support Vector Regression plot
create_plot(predict(model_svr, data.frame(x_test)), y_test, 'Support Vector Machine')

# Random Forest plot
create_plot(predict(model_rf, data.frame(x_test)), y_test, 'Random Forest')

# XGBoost plot
create_plot(predict(model_xgb, D_test), y_test, 'XGBoost')
```

```{r}
# Calculate R-squared and adjusted R-squared for Linear Regression
summary_reg <- summary(reg)
r2 <- summary_reg$r.squared
adjusted_r2 <- summary_reg$adj.r.squared

cat(sprintf("R^2 and Adjusted R^2 Linear Regression: %.3f, %.3f\n", r2, adjusted_r2))

```

-   R-squared (R²):

    -   **Value**: 0.883

    -   **Meaning**: R-squared is the proportion of the variance in the dependent variable that is predictable from the independent variables. In this case, an R² value of 0.883 means that approximately 88.3% of the variability in the response variable can be explained by the linear model. This high R² value suggests that the model has a strong explanatory power and fits the data well.

    Adjusted R-squared:

    -   **Value**: 0.882

    -   **Meaning**: Adjusted R-squared adjusts the R² value for the number of predictors (gives penalty fo high number of features) in the model. It provides a more accurate measure of the model's explanatory power, especially when multiple predictors are involved. An adjusted R² value of 0.882, which is very close to the R² value, indicates that the model’s explanatory power is robust and not inflated by the number of predictors. It suggests that the model generalizes well to new data.

    ```{r}
    # Fit the Generalized Linear Model (Polynomial Regression)
    model_GLM <- lm(y_train ~ ., data = gen_features_train)

    # Get summary of the model
    summary_GLM <- summary(model_GLM)

    # Extract R-squared and adjusted R-squared
    r2_GLM <- summary_GLM$r.squared
    adjusted_r2_GLM <- summary_GLM$adj.r.squared

    cat(sprintf("R^2 and Adjusted R^2 Generalized Linear Model: %.3f, %.3f\n", r2_GLM, adjusted_r2_GLM))

    ```

-   **Adjusted R-squared**:

    -   **Value**: 0.893

    -   **Meaning**: Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It provides a more accurate measure of the model's goodness of fit, especially when comparing models with different numbers of predictors
